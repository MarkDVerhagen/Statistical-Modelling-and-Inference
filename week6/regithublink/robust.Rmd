---
title: "Robust Regression"
author: "Aimee, Roger and Hari"
date: "November 29, 2015"
output: pdf_document
---

### 1. The Laplace distribution

##### 1.1 Write down the likelihood for $\mu$, $\Lambda$

$\prod^{N}_{i=1} \frac{1}{2}\Lambda e^{-\Lambda |t_{i} - \mu|} = \frac{1}{2} \Lambda e^{\sum^{N}_{i=1} -\Lambda |t_{i}-\mu|}$

##### 1.2. Show that

$\mu_{mle} = median(t_{1}, ..., t_{N})$

The likelihood function expands to:

$f(\mu,\Lambda) = \frac{1}{2} \Lambda e^{\sum^{N}_{i=1} -\Lambda |t_{i}-\mu|}$

Maximizing the likelihood function is equivalent to maximizing the log of the likelihood function.

$log(f(\mu,\Lambda)) = Nlog(\frac{1}{2}) + Nlog(\Lambda) + \sum_{n} -\Lambda |t_{n}-\mu|$

Taking the derivate with respect to $\mu$ and set equal to 0:

$-\Lambda \sum_{n} sgn(t_{n} - \mu) = 0$

If $N$ is odd there are $\frac{N-1}{2}$ cases where $t_{i} < \mu$ and for the other $\frac{N-1}{2}$ cases $t_{i} > \mu$ and thus we have an equality with zero and the median of $t_{i}$ is $\mu_{mle}$. If $N$ is even, there is no way to satisfy this equality but we can minimize it by taking either $\frac{t_{i}}{N}$ or $\frac{t_{i}}{N-1}$

##### 1.3. Show that

$\Lambda_{mle} = (\frac{1}{N} \sum^{N}_{i=1} |t_{i}-\mu_{mle}|)^{-1}$ 

Taking the derivative of the likelihood function and setting it equal to zero results in: 

$\frac{N}{\Lambda} =  \sum^{N}_{i=1} |t_{i} - \mu |$

which implies 

$\Lambda = (\frac{1}{N} \sum^{N}_{i=1} |t_{i} - \mu |)^{-1}$

And replacing $\mu$ with $\mu_{mle}$, we get 

$\Lambda_{MLE} = (\frac{1}{N} \sum^{N}_{i=1} |t_{i} - \mu_{mle} |)^{-1}$

##### 1.4. Show that 

$\Sigma_{mle} = 2 \frac{1}{N} \sum_{n} | t_{n} - \mu_{mle} |$

Where $\Sigma := var[t]$ and $var[t] = \frac{2}{\Lambda^{2}}$

Using the equivariance property

$\Sigma_{mle} =  \frac{2}{\Lambda_{mle}^{2}}$

We know that $\Lambda_{MLE} = (\frac{1}{N} \sum^{N}_{i=1} |t_{i} - \mu_{mle} |)^{-1}$. Therefore 

$\Sigma_{mle} = 2 (\frac{1}{N} \sum_{n} |t_{n} - \mu_{mle}|)^{2}$ 

### 2. EM algorithm for robust regression 

##### 2.1. Show that if 

$t_{n} | x_{n},w,q,\eta_{n}$ ~ $N(\phi(x_{n}^{T}w,(\eta_{n}q)^{-1}I))$ and $\eta_{n}$ ~ $Gam(\frac{\nu}{2}, \frac{\nu}{2} - 1)$ then

$\eta_{n} | t_{n},x_{n},w,q$ ~ $Gam(\frac{(\nu+1)}{2},\frac{(\nu + q e_{n}^2)}{2} - 1 )$

with $e_{n} := t_n - \phi(x_{n})^{T}w$

By Bayes, we know that $p(\eta_{n} | t_{n},x_{n},w,q) \propto p(\eta) p(t_{n} | x_{n},w,q,\eta_{n})$

$\eta$ follows the known gamma distribution:
$p(\eta) = \frac{({\frac{\nu}{2} - 1})^\frac{\nu}{2}}{(\frac{\nu}{2} - 1)!} (\nu)^{\frac{\nu}{2} - 1} exp^{ -(\frac{\nu}{2} - 1) \eta_{n} }$

The conditional distribution of t follows the known normal distribution:
$p(t_{n} | x_{n},w,q,\eta_{n}) = \frac{1}{(2 \pi)^{\frac{1}{2}}} (q \eta_{n})^\frac{1}{2} exp^{ - \frac{1}{2} e_{n}^2 q}$

Which implies that the conditional distribution of $\eta$ is as follows:

$p(\eta_{n} | t_{n},x_{n},w,q) \propto e^{ -(\frac{\nu}{2} - 1) \eta_{n} } e^{ - \frac{1}{2} e_{n}^2 q} *(\nu)^{\frac{\nu}{2} - 1} \eta_{n}^{\frac{1}{2}}$

$p(\eta_{n} | t_{n},x_{n},w,q) \propto exp^{ - (\frac{\nu + q e_{n}^{2}}{2} - 1) \eta_{n}}   \eta_{n}^{(\frac{\nu - 1}{2})}$

Therefore

$\eta_{n} | t_{n},x_{n},w,q$ ~ $Gam(\frac{(\nu+1)}{2},\frac{(\nu + q e_{n}^2)}{2} - 1 )$

##### 2.2. Show that 

when $\theta = (w,q)$ and $\theta^{'} = (w^{'},q^{'})$:

$Q(\theta,\theta^{'}) = \frac{N}{2}log(q) - \frac{q}{2}(t - \Phi w)^T)diag(\mathbb{E}[\eta | T,x,\theta^{'}])(t - \Phi w) + C$

where 

$\mathbb{E}[\eta | T,x,\theta^{'}]$ is a vector with elements $\frac{\nu + 1}{\nu + q^{'}(t_{N} - \phi(x_n)^{T}w^{'})^2 - 2}$


$Q(\theta,\theta^{'}) = \int log p(t | \eta, \theta)p( \eta | t, \theta^{'})d\eta$

$Q(\theta,\theta^{'}) = \mathbb{E}(Log(L(\theta | t, \eta)))$

$Q(\theta,\theta^{'}) = \mathbb{E}(Log(L(t | \eta , \theta) p ( \eta, \theta, t)))$

$Q(\theta,\theta^{'}) =  \mathbb{E}(\frac{N}{2}log(q) - \frac{1}{2} (t - \Phi w)^{T} q\eta_{n}I (t - \Phi w) + C)$

$Q(\theta,\theta^{'}) =  \frac{N}{2}log(q) - \frac{q}{2}diag(\mathbb{E}[\eta | T,x,\theta^{'}]) + C$

and 

$\mathbb{E}[\eta | t,x,\theta^{'}] = \frac{(\frac{(\nu+1)}{2})}{(\frac{(\nu + q^{'} (t_{n} - \phi(x_{n})^{t} w^{'})^2 - 2)}{2})}$

therefore

$\mathbb{E}[\eta | t,x,\theta^{'}] = \frac{{(\nu+1)})}{(\nu + q^{'} (t_{n} - \phi(x_{n})^{t} w^{'})^2 - 2))}$

### 3. R exercise 

##### 3.1. 

The following graphs compare the coefficients for both plain vanilla MLE estimators and the ones obtained by a robust regression. As can be observed, the differences are very small, indicating that there are no outliers in the data leveraging the standard MLE estimators. 

```{r, echo=FALSE}
data <- read.csv("~/Projects/data_files/synthetic_regression.txt",sep="", nrows=300)
data = data[,c(1:31)]

# create feature matrix and t

phi = as.matrix(cbind(1,data[,c(2:31)]))
t = as.vector(data[,1])

# computing coefficients, std errors and deviance for basic MLE regression

N <- nrow(data)
beta <- as.vector(summary(lm(t ~ phi))$coefficients[,1])
std.e <- as.vector(summary(lm(t ~ phi))$coefficients[,2])
residuals <- as.vector(lm(t ~ phi)$residuals)

plotting.betas.mle = data.frame(Coefficients = beta, Std.Error = std.e)
for (i in 1:31) {ifelse(i == 1, row.names(plotting.betas.mle)[i] <- "Intercept", row.names(plotting.betas.mle)[i] <- i - 1)}

# computing robust coefficients, std errors and deviance 

iter <- 100
nu <- 10
q <- 1/var(t) # setting initial value for q that will make covergence faster
 
# eta <- rep(1, nrow(phi))
log.like <- rep(0, iter)
Q <- rep(0, iter)
error <- t - mean(t)


for(i in 1:iter){
  
  # second step of EM algorithm: expected value of eta with current w and q
  eta <- as.vector((nu + 1)/(nu + q*(error^2) - 2))
  
  # first step of EM algorithm: estimate current q and w 
  W <- solve(t(phi) %*% diag(eta) %*% phi , t(phi) %*% diag(eta) %*% t)
  error <- as.vector(t - phi %*% W)
  q <- as.numeric(solve((1/N)*t(error) %*% diag(eta) %*% error))
  
  log.like[i] <- (N/2)*log(q) + (-q/2)*(t(error) %*% diag(eta) %*% error) 
  
  Q[i] <- q
  
  if(i > 1){
    if(log.like[i] - log.like[i-1] < 1e-4){
      cat("number of iterations",i)
      break
    }
  }
}

# library(MASS)
# r.result = rlm(t ~ phi[,-1], weigths = eta) 

log.like <- log.like[1:i]
Q <- Q[1:i]

#W
#ematrix <- matrix(0,31,31)
#for(j in 1:N){
 # c <- as.numeric((nu + 1)*(nu - 2 - q*(e[j]^2))/((nu + q*(e[j]^2) -2)^2))
  #summ <- c*X[j,] %*% t(X[j,])
  #ematrix = ematrix + summ
#}

exp.eta = as.numeric((nu + 1)*(nu - 2 - q*(error^2))/((nu + q*(error^2) -2)^2))
precision = q*(t(phi) %*% diag(exp.eta) %*% phi)
cov.matrix <- solve(precision)
std.e.rob = sqrt(diag(cov.matrix))
beta.rob = W

plotting.betas = data.frame(Coefficients = beta.rob, Std.Error = std.e.rob)
for (i in 1:31) {ifelse(i == 1, row.names(plotting.betas)[i] <- "Intercept", row.names(plotting.betas)[i] <- i - 1)}
  
#robust.se <- data.frame(value = W,se = sqrt(diag(var)))
#row.names(robust.se)[1] <- "Intercept"


library(ggplot2)

robust.plot <- ggplot() + geom_point(data = plotting.betas, 
                           aes(x = factor(row.names(plotting.betas),
                                          levels = row.names(plotting.betas)), y = Coefficients),
                           colour = 'blue', size = 3) + 
  
     geom_errorbar(data = plotting.betas, 
                aes(x = factor(row.names(plotting.betas),
                               levels = row.names(plotting.betas)), 
                    y = Coefficients, ymax = Coefficients + 1.96*std.e.rob, ymin=Coefficients - 1.96*std.e.rob),
                colour = 'red', width = 0.4) + 
     labs(x= "Regressors",y="Coefficient +/- 1.96 standard error",
       title = "Robust")

robust.plot2 <- ggplot() + geom_point(data = plotting.betas, 
                                     aes(x = Coefficients, 
                                         y = factor(row.names(plotting.betas), levels = row.names(plotting.betas))),
                                     colour = 'black', size = 3) + 
  
  geom_errorbarh(data = plotting.betas, 
                aes(x = Coefficients, xmax = Coefficients + 1.96*std.e.rob, xmin=Coefficients - 1.96*std.e.rob, 
                    y = factor(row.names(plotting.betas), levels = row.names(plotting.betas))),
                colour = 'black', width = 0.4) + 
  labs(x= "Regressors",y="Coefficient +/- 1.96 standard error",
       title = "Robust")

mle.plot <- ggplot() + geom_point(data = plotting.betas.mle, 
                                  aes(x = factor(row.names(plotting.betas.mle),
                                                 levels = row.names(plotting.betas.mle)), y = Coefficients),
                                  colour = 'blue', size = 3) + 
  
  geom_errorbar(data = plotting.betas.mle, 
                aes(x = factor(row.names(plotting.betas.mle),
                               levels = row.names(plotting.betas.mle)), 
                    y = Coefficients, ymax = Coefficients + 1.96*std.e, ymin=Coefficients - 1.96*std.e),
                colour = 'red', width = 0.4) + 
  labs(x= "Regressors",y="Coefficient +/- 1.96 standard error",
       title = "MLE")

mle.plot2 <- ggplot() + geom_point(data = plotting.betas.mle, 
                                     aes(x = Coefficients, 
                                         y = factor(row.names(plotting.betas.mle), levels = row.names(plotting.betas.mle))),
                                     colour = 'black', size = 3) + 
  
  geom_errorbarh(data = plotting.betas.mle, 
                 aes(x = Coefficients, xmax = Coefficients + 1.96*std.e.rob, xmin=Coefficients - 1.96*std.e.rob, 
                     y = factor(row.names(plotting.betas.mle), levels = row.names(plotting.betas.mle))),
                 colour = 'black', width = 0.4) + 
  labs(x= "Regressors",y="",
       title = "MLE")


multiplot <- function(..., plotlist = NULL, file, cols = 1, layout = NULL) {
  require(grid)
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  numPlots = length(plots)
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel ncol: Number of columns of plots nrow: Number of rows
    # needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols,
                     nrow = ceiling(numPlots/cols))
  }
  if (numPlots == 1) {
    print(plots[[1]])
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col))
    }
  }
}

multiplot(robust.plot2, mle.plot2, col=2, layout = matrix(c(1,2), nrow=1, byrow=TRUE))
```

#### 3.2

The following graph plots the deviance residuals for both the standard Gaussian model and the robust regression. 

```{r, echo=FALSE, message= FALSE, warning= FALSE}
dev.res.mle <- (residuals^2)/var(t)
dev.res.rob = (error^2)*(q*eta)

layout(matrix(c(1,2), 1, 2))
plot(dev.res.mle, ylab = "MLE deviance", xlab = "Observation") + 
     abline(a = quantile(dev.res.mle,0.99),b= 0)

plot(dev.res.rob, ylab = "Robust deviance", xlab = "Observation") +
     abline(a = quantile(dev.res.mle,0.99),b= 0)
```

#### 3.3.

The following graph plots the log-likelihood at each iteration of the EM algorithm. To build it, we have included a counter in the EM algorithm recording the value of the log-likelihood at each iteration, along with a rule to indicate the algorithm that enough convergence was reached. Specifically, we have indicated that the algorithm stops when the increments in the log-likelihood are smaller than 0.0001. We can see that convergence is reached at about the tenth iteration. 

```{r, echo=FALSE, message= FALSE, warning= FALSE}
plot(log.like, ylab = "Log-Likelihood at each iteration", xlab = "Iteration", type="b")
```

##### 3.4.

To find an estimate of the degrees of freedom for the latent variable $\eta$, we have applied the EM algorithm for a range of values for nu, looking for the moment at which increases in nu cease to increase significantly the log-likelihood. We can see in the graph produced below that for values of nu greater than around 20, there is barely any improvement in the likelihood.  

```{r, echo=FALSE, message= FALSE, warning= FALSE}
N <- 300
nu <- seq(2.1,300,0.2)
log.like <- rep(200)

for(j in 1:200){
  W <- plotting.betas$value
  q <- 1/var(t)
  error <- t - mean(t)
  for(i in 1:10){
    eta <- as.vector((nu[j] + 1)/(nu[j] + q*(error^2) - 2))
    
    W <- solve(t(phi) %*% diag(eta) %*% phi , t(phi) %*% diag(eta) %*% t)
    e <- as.vector(t - phi %*% W)
    q <- as.numeric(solve((1/N)*t(error) %*% diag(eta) %*% error))
    
    log.like[j] <- (-1/2)*(t(error) %*% diag(q*eta) %*% e) + (N/2)*log(q)
  }
   if(j > 1){
    if(log.like[j] - log.like[j-1] < 1e-4){
      cat("Convergence reached at nu =",nu[j])
      break
   }
  }
}

log.like <- log.like[1:j]
nu = nu[1:j]
layout(matrix(1, 1, 1))
plot(y=log.like, x=nu, ylab = "Log-Likelihood", xlab = "Value of nu", type="b")
```